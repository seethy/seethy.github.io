<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[openssl常用命令]]></title>
      <url>/2023/11/25/openssl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      <content type="html"><![CDATA[<h3 id="查看ca有效期"><a href="#查看ca有效期" class="headerlink" title="查看ca有效期"></a>查看ca有效期</h3><p> openssl x509 -in ca.pem -dates</p>
<h3 id="生成私钥"><a href="#生成私钥" class="headerlink" title="生成私钥"></a>生成私钥</h3><p> openssl  genrsa -out  rsa.key 1024</p>
<blockquote>
<p>$ openssl genrsa -h<br>&#x2F;**<br>usage: genrsa [args] [numbits]<br>-des           生成的私钥采用DES算法加密<br> -des3          生成的私钥采用DES3算法加密 (168 bit key)<br>  -seed          encrypt PEM output with cbc seed<br>    -aes128, -aes192, -aes256<br>    以上几个都是对称加密算法的指定，因为我们长期会把私钥加密，避免明文存放<br>     -out file       私钥输出位置<br>      -passout arg    输出文件的密码，如果我们指定了对称加密算法，也可以不带此参数，会有命令行提示你输入密码<br>      **&#x2F;</p>
</blockquote>
<h3 id="生成公钥"><a href="#生成公钥" class="headerlink" title="生成公钥"></a>生成公钥</h3><pre><code>    openssl  rsa -pubout -in rsa.key -out pub.key
</code></pre>
<h3 id="公钥-私钥加密"><a href="#公钥-私钥加密" class="headerlink" title="公钥&#x2F;私钥加密"></a>公钥&#x2F;私钥加密</h3><pre><code>    openssl  rsautl -encrypt -in plain.txt -inkey rsa.key -out encrypt.txt
</code></pre>
<h3 id="私钥解密"><a href="#私钥解密" class="headerlink" title="私钥解密"></a>私钥解密</h3><pre><code>     openssl rsautl -decrypt -in encrypt.txt -inkey rsa.key -out plain1.txt
</code></pre>
<h3 id="base64解码"><a href="#base64解码" class="headerlink" title="base64解码"></a>base64解码</h3><pre><code>     cat testencrypt |base64 -d &gt; testencrypt.base64
</code></pre>
<h3 id="将普通私钥转成PKCS-8编码"><a href="#将普通私钥转成PKCS-8编码" class="headerlink" title="将普通私钥转成PKCS#8编码"></a>将普通私钥转成PKCS#8编码</h3><pre><code>     openssl pkcs8 -topk8 -in rsa_private_key.pem -out pkcs8_rsa_private_key.pem -nocrypt
</code></pre>
]]></content>
      
        <categories>
            
            <category> 编程语言 </category>
            
            <category> shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> openssl </tag>
            
            <tag> 密钥 </tag>
            
            <tag> 公钥 </tag>
            
            <tag> 私钥 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[blackbox黑盒监控主机存活，网络时延]]></title>
      <url>/2023/09/09/blackbox%E9%BB%91%E7%9B%92%E7%9B%91%E6%8E%A7%E4%B8%BB%E6%9C%BA%E5%AD%98%E6%B4%BB%EF%BC%8C%E7%BD%91%E7%BB%9C%E6%97%B6%E5%BB%B6/</url>
      <content type="html"><![CDATA[<p>​       有时候，在一个集群网络中，需要持续关注各个边缘节点的网络性能，以保证能够持续提供高可靠性的服务，因此需要有一个工具能够提供持续探测主机存活，网络时延的机制。<br>​       Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测。本章节主要讲述blackbox ICMP主机探活机制，其本质主要是通过ping命令来检测目的主机的连通性。</p>
<p><strong>Blackbox配置：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">icmp:</span></span><br><span class="line">  <span class="attr">prober:</span> <span class="string">icmp</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="string">2s</span></span><br><span class="line">  <span class="attr">icmp:</span></span><br><span class="line">    <span class="attr">preferred_ip_protocol:</span> <span class="string">ip4</span></span><br><span class="line">    <span class="attr">ip_protocol_fallback:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>​    timeout代表探测的超时时间，重点关注preferred_ip_protocol: ip4配置，因为默认是使用ipv6地址探测，从而导致探测失败。<br>​    Blackbox容器部署配置，因为默认alpine基础镜像普通用户是不支持ping操作的，所以需要加上root用户权限</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">runAsNonRoot:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">runAsUser:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>​      prometheus scrape配置如下， 这里的测试环境中，80.80.130.[79-81]这3个节点的网络是正常的，构造了一个不存在的节点80.80.130.200来测试联通性。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-probe-ping-status&#x27;</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">60s</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">tls_config:</span></span><br><span class="line">    <span class="attr">insecure_skip_verify:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">cert_file:</span> <span class="string">/etc/prometheus/secrets/tool-cert/tls.crt</span></span><br><span class="line">    <span class="attr">key_file:</span> <span class="string">/etc/prometheus/secrets/tool-cert/tls.key</span></span><br><span class="line">    <span class="attr">ca_file:</span> <span class="string">/etc/prometheus/secrets/tool-cert/ca.crt</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;80.80.130.79&#x27;</span>,<span class="string">&#x27;80.80.130.80&#x27;</span>,<span class="string">&#x27;80.80.130.81&#x27;</span>,<span class="string">&#x27;80.80.130.200&#x27;</span>]</span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">group:</span> <span class="string">icmp</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">/probe</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">module:</span> [<span class="string">icmp</span>]</span><br><span class="line">  <span class="attr">relabel_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">      <span class="attr">regex:</span> <span class="string">__meta_kubernetes_pod_label_(.+)</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>]</span><br><span class="line">      <span class="attr">target_label:</span> <span class="string">__param_target</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">      <span class="attr">replacement:</span> <span class="string">blackbox-exporter.mon.svc.cluster.local.:6800</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__param_target</span>]</span><br><span class="line">      <span class="attr">target_label:</span> <span class="string">target_url</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">instance</span>]</span><br><span class="line">      <span class="attr">target_label:</span> <span class="string">deviceid</span></span><br><span class="line">      <span class="attr">regex:</span> <span class="string">(.*)</span></span><br><span class="line">      <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">      <span class="attr">action:</span> <span class="string">replace</span></span><br></pre></td></tr></table></figure>
<p>​     查看prometheus结果， 可以看到正常节点的网络时延，80.80.130.200因为网络不通，所以2s超时了。<br>​    </p>
]]></content>
      
        <categories>
            
            <category> 组件服务 </category>
            
            <category> blackbox </category>
            
        </categories>
        
        
        <tags>
            
            <tag> blackbox </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HDFS架构]]></title>
      <url>/2023/09/08/HDFS%E6%9E%B6%E6%9E%84/</url>
      <content type="html"><![CDATA[<p>　　</p>
<p>​    <img src="/images/HDFS%E6%9E%B6%E6%9E%84.jpeg" alt="HDFS架构图"></p>
<p>　　HDFS 采用Master&#x2F;Slave的架构来存储数据，这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分  </p>
<p>　　<strong>1、Client：就是客户端。</strong></p>
<ul>
<li>文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。</li>
<li>与 NameNode 交互，获取文件的位置信息。</li>
<li>与 DataNode 交互，读取或者写入数据。</li>
<li>Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS。</li>
<li>Client 可以通过一些命令来访问 HDFS。</li>
</ul>
<p>　　<strong>2、NameNode：就是 master，它是一个主管、管理者。</strong></p>
<ul>
<li>管理 HDFS 的名称空间</li>
<li>管理数据块（Block）映射信息</li>
<li>配置副本策略</li>
<li>处理客户端读写请求。</li>
</ul>
<p>　　<strong>3、DataNode：就是Slave。NameNode 下达命令，DataNode 执行实际的操作。</strong></p>
<ul>
<li>存储实际的数据块。</li>
<li>执行数据块的读&#x2F;写操作。</li>
</ul>
<p>　　<strong>4、Secondary NameNode：并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。</strong></p>
<ul>
<li>辅助 NameNode，分担其工作量。</li>
<li>定期合并 fsimage和fsedits，并推送给NameNode。</li>
<li>在紧急情况下，可辅助恢复 NameNode。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 组件服务 </category>
            
            <category> HDFS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HDFS特性]]></title>
      <url>/2023/09/07/HDFS%E7%89%B9%E6%80%A7/</url>
      <content type="html"><![CDATA[<p><strong>优点：</strong>　</p>
<p>1、高容错性</p>
<ul>
<li>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</li>
<li>某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。</li>
</ul>
<p>2、适合批处理</p>
<ul>
<li>它是通过移动计算而不是移动数据。</li>
<li>它会把数据位置暴露给计算框架。</li>
</ul>
<p>3、适合大数据处理</p>
<ul>
<li>处理数据达到 GB、TB、甚至PB级别的数据。</li>
<li>能够处理百万规模以上的文件数量，数量相当之大。</li>
<li>能够处理10K节点的规模。</li>
</ul>
<p>4、流式文件访问</p>
<ul>
<li>一次写入，多次读取。文件一旦写入不能修改，只能追加。</li>
<li>它能保证数据的一致性。</li>
</ul>
<p>5、可构建在廉价机器上</p>
<ul>
<li>它通过多副本机制，提高可靠性。</li>
<li>它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。</li>
</ul>
<p><strong>缺点：</strong></p>
<p>1、低延时数据访问</p>
<ul>
<li>比如毫秒级的来存储数据，这是不行的，它做不到。</li>
<li>它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。</li>
</ul>
<p>2、小文件存储</p>
<ul>
<li>存储大量小文件(这里的小文件是指小于HDFS系统的Block大小的文件（默认64M）)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。</li>
<li>小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。</li>
</ul>
<p>3、并发写入、文件随机修改</p>
<ul>
<li>一个文件只能有一个写，不允许多个线程同时写。</li>
<li>仅支持数据 append（追加），不支持文件的随机修改。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 组件服务 </category>
            
            <category> HDFS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GO本地编译&&离线编译]]></title>
      <url>/2023/09/03/GO%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91-%E7%A6%BB%E7%BA%BF%E7%BC%96%E8%AF%91/</url>
      <content type="html"><![CDATA[<ol>
<li>设置环境变量:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> GOROOT=/home/go</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$GOROOT</span>/bin</span><br></pre></td></tr></table></figure>
打开本地代理（在线，可以下载依赖的module）<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">env</span> -w GO111MODULE=on</span><br><span class="line">go <span class="built_in">env</span> -w GOPROXY=https://goproxy.cn,direct</span><br></pre></td></tr></table></figure>
将本地下载的go module $GOROOT&#x2F;bin&#x2F;pkg 拷贝到离线服务器的 $GOROOT&#x2F;bin&#x2F;pkg 下<br>在离线服务器上设置<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">env</span> -w GOPROXY=file:///<span class="variable">$GOROOT</span>/bin/pkg/mod/cache/download</span><br></pre></td></tr></table></figure></li>
<li>编译命令：<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go build -o sql_exporter main.go</span><br><span class="line"><span class="comment"># alpine环境下运行编译命令</span></span><br><span class="line"><span class="comment"># alpine环境下运行需要go env -w CGO_ENABLED=0</span></span><br><span class="line">CGO_ENABLED=0 go build -o sql_exporter main.go</span><br></pre></td></tr></table></figure>
关闭安全性校验： go env -w GOSUMDB&#x3D;off</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 编程语言 </category>
            
            <category> GO </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Go编译 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[解决git:filename too long报错]]></title>
      <url>/2023/09/03/%E8%A7%A3%E5%86%B3git-filename-too-long%E6%8A%A5%E9%94%99/</url>
      <content type="html"><![CDATA[<p>git config –global core.longpaths true</p>
]]></content>
      
        <categories>
            
            <category> 组件服务 </category>
            
            <category> git </category>
            
        </categories>
        
        
        <tags>
            
            <tag> git </tag>
            
            <tag> 故障排查 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SparkSQL数据倾斜性能优化]]></title>
      <url>/2023/09/02/SparkSQL%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p><strong>背景：</strong><br>国内某大牌运营商话单数据量比较大，大概一天2T左右话单，标准话单一个小时需要处理9KW左右的话单量，标准话单处理任务在执行30分钟左右，报“too large frame”或者”Size exceeds Integer.MAX_VALUE”错误，无法正常生成标准话单数据。<br><strong>1小时任务处理数据量：</strong><br>OTT话单 ：9KW<br>8303话单：无<br>内容数据：1千万<br>节目数据：无<br>用户数据：7百万<br>栏目：3<br>频道数据：无<br>节目单，tvod: 无<br>产品信息：100<br><strong>根因分析：</strong><br>spark任务报这2个错，主要是发生在shuffle阶段，因为Spark对每个partition所能包含的数据大小有写死的限制（约为2G），当某个partition包含超过此限制的数据时，就会抛出这类异常。<br>造成此异常的主要原因有:</p>
<ol>
<li>源数据太多，partition分区数太少，导致分配到每个partition上的数据量过多，超过阈值。</li>
<li>数据倾斜，某列的数据分布不均衡，当某个shuffle操作是根据此列数据进行shuffle时，就会造成整个数据集发生倾斜，即某几个partition包含了大量数据，并且其数据大小超过了Spark的限制，而其他partition只包含很少的数据。</li>
</ol>
<p><strong>解决方案：</strong></p>
<ol>
<li>通过调整spark.sql.shuffle.partitions，增加分区数。</li>
<li>消除数据倾斜。<br>spark join主要有以下两种方式：<br>a) Broadcast Hash Join ：当其中一个数据集足够小时，采用Broadcast Hash Join，较小的数据集会被广播到所有Spark的executor上，并转化为一个Hash Table，之后较大数据集的各个分区会在各executor上与Hash Table进行本地的Join，各分区Join的结果合并为最终结果。<br>Broadcast Hash Join 没有Shuffle阶段、效率最高。但为了保证可靠性，executor必须有足够的内存能放得下被广播的数据集，所以当进两个数据集的大小都超过一个可配置的阈值之后，Spark不会采用这种Join。控制这个阈值的参数为spark.sql.autoBroadcastJoinThreshold, 中默认值为10M。<br>b) Sort Merge Join:  将key相同的记录重分配同一个executor上，不同的是，在每个executor上，不再构造哈希表，而是对两个分区进行排序，然后用两个下标同时遍历两个分区，如果两个下标指向的记录key相同，则输出这两条记录，否则移动key较小的下标。<br>Sort Merge Join也有Shuffle阶段，因此效率同样不如Broadcast Hash Join。在内存使用方面，因为不需要构造哈希表，需要的内存比Hash Join要少。<br>所以数据倾斜一般发生在sort merge join过程，大表跟大表关联一般建议使用sort merge join，大表的数据倾斜，可以采用将倾斜键<em>随机数(比如100以内的随机数), 另外一个表对应的键</em>100这种以空间换效率的方式； 大表跟小表关联，一般建议将小表cache, 然后通过broadcast的方式分发到各executor中提高处理性能，而且也可以避免数据倾斜的情况。</li>
</ol>
<p><strong>排查和优化过程：</strong><br>因为话单原始数据量比较大，一开始怀疑默认分区数200不够，调大到1000，spark.sql.shuffle.partitions&#x3D;1000，问题没有解决。<br>检查报错的spark sql语句，主要集中在 OTT话单同时左关联用户数据（通过usercode字段字段关联），内容数据（通过contentcode字段关联），栏目数据（通过columncode字段关联），产品信息（通过productcode字段关联）这4个数据表，通过分析spark UI 的任务执行情况，确定应该是发生数据倾斜。<br>检查用户表里的usercode分布，发现10%话单中usercode为空，在ETL过滤掉usercode为空的话单后，问题没有解决； 非空的usercode基本上分布比较均匀。<br>检查内容数据的分布，发现top5的内容数据量都在1kw 左右，数据倾斜比较严重，但是小时话单经过排查后的内容数总共就12w左右，所以修改代码在关联之前将内容先过滤下，然后再cache table, 充分利用broadcast hash join的优势，不shuffle。将spark.sql.autoBroadcastJoinThreshold配置成500M, 但是经过调整后，问题还是没有解决，还是报同样的错误。凌晨数据量比较小的情况下，偶尔任务能够成功，但是发现生成的结果数据文件差异很大，大文件甚至能够达到5G左右。<br>通过对sql语句进行explain分析，发现左关联的内容表，栏目表等没有通broadcast hash join还是sort merge join， 怀疑创建的内容表，栏目表等临时表 cache方式有问题。<br>针对spark sql broadcast的条件, 于是做了以下的测试：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> TEMPORARY <span class="keyword">view</span> view1 asselect d.columncode,c.contentcodefrom vinsight_common_vd_vas_all_program_spark cleft <span class="keyword">join</span> vinsight_common_vd_vas_all_column_spark don c.columncode<span class="operator">=</span>d.columncode;cache <span class="keyword">table</span> cache1 <span class="keyword">as</span> <span class="keyword">select</span> d.columncode,c.contentcodefrom vinsight_common_vd_vas_all_program_spark cleft <span class="keyword">join</span> vinsight_common_vd_vas_all_column_spark don c.columncode<span class="operator">=</span>d.columncode;<span class="keyword">create</span> <span class="keyword">table</span> tab1 asselect d.columncode,c.contentcodefrom vinsight_common_vd_vas_all_program_spark cleft <span class="keyword">join</span> vinsight_common_vd_vas_all_column_spark don c.columncode<span class="operator">=</span>d.columncode;explain <span class="keyword">select</span><span class="comment">/*+ MAPJOIN(b) */</span> a.usercode, b.contentcodefrom vinsight_common_md_fact_standardcdr_spark a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> tab1 bon a.contentcode <span class="operator">=</span> b.contentcodewhere a.p_date<span class="operator">&gt;</span><span class="string">&#x27;2021-12-01&#x27;</span>;<span class="operator">+</span>cache1,  view1<span class="operator">|</span> <span class="operator">=</span><span class="operator">=</span> Physical Plan <span class="operator">=</span><span class="operator">=</span><span class="operator">*</span>Project [usercode#<span class="number">19861</span>, contentcode#<span class="number">19683</span>]<span class="operator">+</span><span class="operator">-</span> <span class="operator">*</span>BroadcastHashJoin [contentcode#<span class="number">19870</span>], [contentcode#<span class="number">19683</span>], LeftOuter, BuildRight</span><br><span class="line">   :<span class="operator">-</span> HiveTableScan [usercode#<span class="number">19861</span>, contentcode#<span class="number">19870</span>], HiveTableRelation `zxvmax`.`vinsight_common_md_fact_standardcdr_spark`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [cdrtype#<span class="number">19860</span>, usercode#<span class="number">19861</span>, profilecode#<span class="number">19862</span>, begintime#<span class="number">19863</span>, endtime#<span class="number">19864</span>, timelen#<span class="number">19865</span>L, relativeurl#<span class="number">19866</span>, servicetype#<span class="number">19867</span>, columncode#<span class="number">19868</span>, columnname#<span class="number">19869</span>, contentcode#<span class="number">19870</span>, contentname#<span class="number">19871</span>, physicalcontentid#<span class="number">19872</span>, channelcode#<span class="number">19873</span>, channelname#<span class="number">19874</span>, prevuecode#<span class="number">19875</span>, prevuename#<span class="number">19876</span>, programcode#<span class="number">19877</span>, programname#<span class="number">19878</span>, seriesheadcode#<span class="number">19879</span>, seriesheadname#<span class="number">19880</span>, cpcode#<span class="number">19881</span>, cpname#<span class="number">19882</span>, telecomcode#<span class="number">19883</span>, ... <span class="number">38</span> more fields], [p_provincecode#<span class="number">19922</span>, p_date#<span class="number">19923</span>, p_hour#<span class="number">19924</span>], [isnotnull(p_date#<span class="number">19923</span>), (p_date#<span class="number">19923</span> <span class="operator">&gt;</span> <span class="number">2021</span><span class="number">-12</span><span class="number">-01</span>)]</span><br><span class="line">   <span class="operator">+</span><span class="operator">-</span> BroadcastExchange HashedRelationBroadcastMode(List(input[<span class="number">0</span>, string, <span class="literal">true</span>]))</span><br><span class="line">      <span class="operator">+</span><span class="operator">-</span> InMemoryTableScan [contentcode#<span class="number">19683</span>]</span><br><span class="line">            <span class="operator">+</span><span class="operator">-</span> InMemoryRelation [columncode#<span class="number">19730</span>, contentcode#<span class="number">19683</span>], <span class="literal">true</span>, <span class="number">10000</span>, StorageLevel(disk, memory, deserialized, <span class="number">1</span> replicas), `cache1`</span><br><span class="line">                  <span class="operator">+</span><span class="operator">-</span> <span class="operator">*</span>Project [columncode#<span class="number">19730</span>, contentcode#<span class="number">19683</span>]</span><br><span class="line">                     <span class="operator">+</span><span class="operator">-</span> SortMergeJoin [columncode#<span class="number">19704</span>], [columncode#<span class="number">19730</span>], LeftOuter</span><br><span class="line">                        :<span class="operator">-</span> <span class="operator">*</span>Sort [columncode#<span class="number">19704</span> <span class="keyword">ASC</span> NULLS <span class="keyword">FIRST</span>], <span class="literal">false</span>, <span class="number">0</span></span><br><span class="line">                        :  <span class="operator">+</span><span class="operator">-</span> Exchange hashpartitioning(columncode#<span class="number">19704</span>, <span class="number">200</span>)</span><br><span class="line">                        :     <span class="operator">+</span><span class="operator">-</span> HiveTableScan [contentcode#<span class="number">19683</span>, columncode#<span class="number">19704</span>], HiveTableRelation `zxvmax`.`vinsight_common_vd_vas_all_program_spark`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [opflag#<span class="number">19676</span>, optime#<span class="number">19677</span>, programcode#<span class="number">19678</span>, bocode#<span class="number">19679</span>, langtype#<span class="number">19680</span>, programname#<span class="number">19681</span>, programtype#<span class="number">19682</span>, contentcode#<span class="number">19683</span>, seriesprogramcode#<span class="number">19684</span>, programhead#<span class="number">19685</span>, mediaservices#<span class="number">19686</span>, ratingid#<span class="number">19687</span>, onlinetime#<span class="number">19688</span>, offlinetime#<span class="number">19689</span>, createtime#<span class="number">19690</span>, countryname#<span class="number">19691</span>, telecomcode#<span class="number">19692</span>, mediacode#<span class="number">19693</span>, seriesnum#<span class="number">19694</span>, posterfilelist#<span class="number">19695</span>, wggenre#<span class="number">19696</span>, wgtags#<span class="number">19697</span>, wgkeywords#<span class="number">19698</span>, description#<span class="number">19699</span>, ... <span class="number">27</span> more fields], [p_provincecode#<span class="number">19727</span>]</span><br><span class="line">                        <span class="operator">+</span><span class="operator">-</span> <span class="operator">*</span>Sort [columncode#<span class="number">19730</span> <span class="keyword">ASC</span> NULLS <span class="keyword">FIRST</span>], <span class="literal">false</span>, <span class="number">0</span></span><br><span class="line">                           <span class="operator">+</span><span class="operator">-</span> Exchange hashpartitioning(columncode#<span class="number">19730</span>, <span class="number">200</span>)</span><br><span class="line">                              <span class="operator">+</span><span class="operator">-</span> HiveTableScan [columncode#<span class="number">19730</span>], HiveTableRelation `zxvmax`.`vinsight_common_vd_vas_all_column_spark`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [opflag#<span class="number">19728</span>, optime#<span class="number">19729</span>, columncode#<span class="number">19730</span>, bocode#<span class="number">19731</span>, langtype#<span class="number">19732</span>, subjectname#<span class="number">19733</span>, parentcode#<span class="number">19734</span>, subjecttype#<span class="number">19735</span>, description#<span class="number">19736</span>, mediacode#<span class="number">19737</span>, telecomcode#<span class="number">19738</span>, advertiseflag#<span class="number">19739</span>, posterfilelist#<span class="number">19740</span>, columnlock#<span class="number">19741</span>, subexist#<span class="number">19742</span>, updatetime#<span class="number">19743</span>, p_day#<span class="number">19744</span>], [p_provincecode#<span class="number">19745</span>]  <span class="operator">|</span></span><br><span class="line">							  tab1<span class="comment">-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+| == Physical Plan ==*Project [usercode#20108, contentcode#20173]+- *BroadcastHashJoin [contentcode#20117], [contentcode#20173], LeftOuter, BuildRight</span></span><br><span class="line">   :<span class="operator">-</span> HiveTableScan [usercode#<span class="number">20108</span>, contentcode#<span class="number">20117</span>], HiveTableRelation `zxvmax`.`vinsight_common_md_fact_standardcdr_spark`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [cdrtype#<span class="number">20107</span>, usercode#<span class="number">20108</span>, profilecode#<span class="number">20109</span>, begintime#<span class="number">20110</span>, endtime#<span class="number">20111</span>, timelen#<span class="number">20112</span>L, relativeurl#<span class="number">20113</span>, servicetype#<span class="number">20114</span>, columncode#<span class="number">20115</span>, columnname#<span class="number">20116</span>, contentcode#<span class="number">20117</span>, contentname#<span class="number">20118</span>, physicalcontentid#<span class="number">20119</span>, channelcode#<span class="number">20120</span>, channelname#<span class="number">20121</span>, prevuecode#<span class="number">20122</span>, prevuename#<span class="number">20123</span>, programcode#<span class="number">20124</span>, programname#<span class="number">20125</span>, seriesheadcode#<span class="number">20126</span>, seriesheadname#<span class="number">20127</span>, cpcode#<span class="number">20128</span>, cpname#<span class="number">20129</span>, telecomcode#<span class="number">20130</span>, ... <span class="number">38</span> more fields], [p_provincecode#<span class="number">20169</span>, p_date#<span class="number">20170</span>, p_hour#<span class="number">20171</span>], [isnotnull(p_date#<span class="number">20170</span>), (p_date#<span class="number">20170</span> <span class="operator">&gt;</span> <span class="number">2021</span><span class="number">-12</span><span class="number">-01</span>)]</span><br><span class="line">   <span class="operator">+</span><span class="operator">-</span> BroadcastExchange HashedRelationBroadcastMode(List(input[<span class="number">0</span>, string, <span class="literal">true</span>]))</span><br><span class="line">      <span class="operator">+</span><span class="operator">-</span> HiveTableScan [contentcode#<span class="number">20173</span>], HiveTableRelation `zxvmax`.`tab1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [columncode#<span class="number">20172</span>, contentcode#<span class="number">20173</span>]  <span class="operator">|</span></span><br></pre></td></tr></table></figure>
<p><strong>总结:</strong><br>cache1,view1 这2种方式生成的执行计划是一样的，但是这种临时表在跟大表左关联时，还是会从原来的临时表做分解，导致还是存在SortMergeJoin, shuffle过程的。<br>采用创建新表这种临时表方式，然后使用hint  MAPJOIN明确使用boradcast方式可以广播小表。<br>最终修改sparksql为第二种方式后，任务运行结果文件比较均衡，基本上每个分区文件在50M以下，任务时间也控制在15分钟以内完成。</p>
]]></content>
      
        <categories>
            
            <category> 组件服务 </category>
            
            <category> sparksql </category>
            
        </categories>
        
        
        <tags>
            
            <tag> sparksql </tag>
            
            <tag> 数据倾斜 </tag>
            
            <tag> 性能优化 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[volatile]]></title>
      <url>/2023/08/27/volatile/</url>
      <content type="html"><![CDATA[<p>volatile 是一个类型修饰符。volatile 的作用是作为指令关键字，确保本条指令不会因编译器的优化而省略</p>
<p><strong>volatile 的特性</strong></p>
<ol>
<li><p>保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。（实现可见性）</p>
</li>
<li><p>禁止进行指令重排序。（实现有序性）</p>
</li>
<li><p>volatile 只能保证对单次读&#x2F;写的原子性。i++ 这种操作不能保证原子性。关于volatile 原子性可以理解为把对volatile变量的单个读&#x2F;写，看成是使用同一个锁对这些单个读&#x2F;写操作做了同步。</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 编程语言 </category>
            
            <category> JAVA </category>
            
            <category> 多线程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 多线程 </tag>
            
            <tag> JAVA </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
